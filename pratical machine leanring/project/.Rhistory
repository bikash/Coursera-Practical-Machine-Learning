### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
install.packages("TeachingDemos")
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Simulation & Modeling of Hidden Markov Model (HMM)
library(TeachingDemos)
library(HMM)
library(ggplot2)
set.seed(1)
### Define our variables
TPM <- matrix(c(.95, .05,
.1, .9), 2, byrow = TRUE)
EPM <- matrix(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6,
1/10, 1/10, 1/10, 1/10, 1/10, 1/2), 2, byrow = TRUE)
simulations <- 500
### Create a dataframe to hold our results
dice <- rep(NA, simulations)
number <- rep.int(0, simulations)
results <- data.frame(dice, number)
### Simulate
# Assume we start with a fair dice
state <- "FAIR"
for (i in 1:simulations) {
if (state == "FAIR") {
# Check to see if we're staying with a FAIR dice
p <- runif(1)
if (p <= TPM[1,2]) {
# If not, roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember new state
state <- "LOADED"
}
else {
# Roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember old state
state <- "FAIR"
}
}
if (state == "LOADED") {
# Check to see if we're staying with a LOADED dice
p <- runif(1)
if (p < TPM[2,1]) {
# If not, roll fair dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[1,])[[1]]
# Remember new state
state <- "FAIR"
}
else {
# Roll loaded dice
roll <- dice(rolls = 1, ndice = 1, sides = 6, load = EPM[2,])[[1]]
# Remember old state
state <- "LOADED"
}
}
# Save dice roll and state
results[i, 1] <- state
results[i, 2] <- roll
}
### Modeling
# Create hmm using our TPM/EPM
hmm <- initHMM(c("FAIR", "LOADED"), c(1, 2, 3, 4, 5, 6),
transProbs = TPM, emissionProbs = EPM)
# Pull in results from the simulation
obs <- results[, 2]
# Save Viterbi/Posterior predictions as a new column
results$viterbi <- viterbi(hmm, obs)
results$posterior <- posterior(hmm, obs)[1, ]
results$posterior[results$posterior >= 0.5] <- "FAIR"
results$posterior[results$posterior < 0.5] <- "LOADED"
# Check out results
table(results$dice)
table(results$viterbi)
table(results$posterior)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
p2 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = viterbi), color = "#00BFC4") +
xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Viterbi Predictions")
p3 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice), color = "#F8766D") +
geom_point(aes(y = posterior), color = "#00BFC4") +
xlab("Dice Roll (in sequence)") + ylab("State") +
ggtitle("Posterior Predictions")
grid.arrange(p1, p2, p3, ncol = 1)
### Plot predictions with true sequence
p1 <- ggplot(aes(x = seq_along(dice)), data = results) +
geom_point(aes(y = dice)) +
ylab("State") + xlab("Dice Roll (In Sequence)") + ylab("State") +
ggtitle("Actual Results")
head(table2)
library(DMwR)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
trainIndex
head(trainIndex)
head(adData)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
testIndex
createDataPartition(diagnosis, p = 0.50,list=FALSE)
head(createDataPartition(diagnosis, p = 0.50,list=FALSE))
createDataPartition(diagnosis,p=0.5,list=FALSE)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
head(training)
tail(training)
head(testing)
tail(testing)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(trainig)
head(training
)
## for turning continuous covariates into factors). What do you notice in these plots?
library(Hmisc)
library(plyr)
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
SplitOn
splitOn
plot(training$CompressiveStrength, col = splitOn)
#Load the Alzheimer's disease data using the commands:
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
answers = rep("A", 20)
answers
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data <- segmentationOriginal
set.seed(125)
inTrain <- data$Case == "Train"
trainData <- data[inTrain, ]
testData <- data[!inTrain, ]
cartModel <- train(Class ~ ., data=trainData, method="rpart")
cartModel$finalModel
plot(cartModel$finalModel, uniform=T)
text(cartModel$finalModel, cex=0.8)
library(pgmm)
data(olive)
olive = olive[,-1]
olive
newdata = as.data.frame(t(colMeans(olive)))
newdata
library(randomForest)
model <- train(Area ~ ., data = olive, method = "rpart2")
model
predict(model, newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
logitModel <-train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial"))
logitModel <-train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial")
logitModel
predtrain = predict(logitModel, newdata=trainSA)
predtest = predict(logitModel, newdata=testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
missClass(trainSA$chd,predtrain) #
missClass(testSA$chd,predtest) #0.277
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
a
b <- varImp(a)
order(b)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
a
order(varImp(a), decreasing=T)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
library(caret)
library(doMC)
library(randomForest)
registerDoMC(cores = 4)
## point to the project folder
setwd("/Users/bikash/repos/Coursera-Practical-Machine-Learning/project/")
print("Loading Data...")
train <- read.csv('pml-training.csv',header=TRUE,stringsAsFactors = F,na.strings=c('NA','','#DIV/0!'))
test <- read.csv('pml-testing.csv',header=TRUE,stringsAsFactors = F,na.strings=c('NA','','#DIV/0!'))
## drop first 7 column
train<-train[,-seq(1:7)]
test<-test[,-seq(1:7)]
## Cleaning the data: dropping NA value
NA_d <- apply(train,2,function(x) {sum(is.na(x))})
train <- train[,which(NA_d == 0)]
NA_d <- apply(test,2,function(x) {sum(is.na(x))})
test <- test[,which(NA_d == 0)]
dim(train)
dim(test)
knitr::knit2html('project.Rmd')
use warnings()
warnings()
predictRF<-predict(processRF,test1)
knitr::knit2html('project.Rmd')
knitr::knit2html('project.Rmd')
```
knitr::knit2html('project.Rmd')
knitr::knit2html('project.Rmd')
